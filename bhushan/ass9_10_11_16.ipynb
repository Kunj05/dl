{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWLfmtvF7G-j",
        "outputId": "3b74d801-69d8-4101-f89e-ec106edbcbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# a) Import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "# ==========================================================\n",
        "# Step 1: Load the Pre-trained CNN Model (VGG16 without top)\n",
        "# ==========================================================\n",
        "base_model = VGG16(weights='vgg16_weights.h5',\n",
        "                   include_top=False,\n",
        "                   input_shape=(224, 224, 3))\n",
        "\n",
        "print(\"âœ… Pre-trained VGG16 model loaded successfully.\")\n",
        "# ==========================================================\n",
        "# Step 2: Freeze lower convolutional layers\n",
        "# ==========================================================\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(\"âœ… Base layers frozen.\")\n",
        "\n",
        "# ==========================================================\n",
        "# Step 3: Add custom classifier (fully connected layers)\n",
        "# ==========================================================\n",
        "# model = Sequential([\n",
        "#     base_model,\n",
        "#     Flatten(),\n",
        "#     Dense(256, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(102, activation='softmax')\n",
        "# ])\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(as many prersent in dataset or use 4, activation='softmax')\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# Step 4: Compile the model\n",
        "# ==========================================================\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# ==========================================================\n",
        "# Step 5: Prepare the dataset (images in subfolders)\n",
        "# ==========================================================\n",
        "data_dir = \"/content/drive/MyDrive/object\"  # your dataset folder\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    #rescale: scales pixel values from [0,255] â†’ [0,1] for stable training.\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # 80% training, 20% validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "print(\"âœ… Dataset loaded successfully.\")\n",
        "\n",
        "\n",
        "print(\"Number of classes:\", train_generator.num_classes)\n",
        "\n",
        "# ==========================================================\n",
        "# Step 6: Train the classifier layers\n",
        "# ==========================================================\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=5\n",
        ")\n",
        "# ==========================================================\n",
        "# Step 7: Fine-tune (unfreeze top layers)\n",
        "# ==========================================================\n",
        "# Unfreeze last 4 convolutional layers\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Re-compile with smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"ðŸ”„ Fine-tuning top layers...\")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=5\n",
        ")\n",
        "# ==========================================================\n",
        "# Step 8: Evaluate and Save the model\n",
        "# ==========================================================\n",
        "loss, acc = model.evaluate(val_generator)\n",
        "print(f\"\\nâœ… Final Validation Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "model.save(\"vgg16_transfer_learning_final.h5\")\n",
        "print(\"âœ… Model saved successfully as vgg16_transfer_learning_final.h5\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x_test, y_test = next(val_generator)\n",
        "import numpy as np\n",
        "# Make predictions\n",
        "predicted_value = model.predict(x_test)\n",
        "\n",
        "# Get label names\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Show example\n",
        "n = 22  # choose any image index\n",
        "plt.imshow(x_test[n])\n",
        "plt.axis('off')\n",
        "plt.title(f\"Predicted: {labels[np.argmax(predicted_value[n])]}\\nActual: {labels[np.argmax(y_test[n])]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transfer Learning for object detection/classification leverages the pre-trained convolutional layers of a deep CNN (such as VGG16) that has already learned powerful feature maps from a large dataset like ImageNet. Instead of training a CNN from scratch, which is expensive and requires massive data, we load the pre-trained model without its top classifier and freeze its convolutional layers so that previously learned low-level and mid-level features (edges, textures, shapes) are preserved. On top of this frozen base network, we attach a custom classifier consisting of dense layers trained specifically on the new dataset. After initial training, we perform fine-tuning by unfreezing the top few convolution layers and training them with a very low learning rate to adapt higher-level feature representations to the new domain. This two-stage procedureâ€”feature extraction followed by fine-tuningâ€”enables high accuracy with limited data and significantly reduces training time. The final model can classify or detect objects effectively using the combined knowledge of a pre-trained backbone and task-specific classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
