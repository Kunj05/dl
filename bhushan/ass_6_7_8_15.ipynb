{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EtSTBpfhddb"
      },
      "source": [
        "Implement the Continuous Bag of Words (CBOW) Model for the given (textual document 1) using the below steps: a. Data preparation b. Generate training data c. Train model d. Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3hRAmD3Thej3",
        "outputId": "c4a1d21f-6825-45e6-fb9f-f4408e9fc8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded text length: 1193\n",
            "Total words: 88\n",
            "Vocabulary size: 60\n",
            "Example Pair: ([32, 52, 19, 42], 43)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.0342 - loss: 4.0937\n",
            "Epoch 2/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0657 - loss: 4.0903\n",
            "Epoch 3/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0657 - loss: 4.0879\n",
            "Epoch 4/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0868 - loss: 4.0856\n",
            "Epoch 5/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1262 - loss: 4.0831\n",
            "Epoch 6/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1238 - loss: 4.0812\n",
            "Epoch 7/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1238 - loss: 4.0787\n",
            "Epoch 8/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1947 - loss: 4.0761 \n",
            "Epoch 9/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1974 - loss: 4.0737\n",
            "Epoch 10/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2158 - loss: 4.0708\n",
            "Epoch 11/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2262 - loss: 4.0687\n",
            "Epoch 12/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2078 - loss: 4.0652\n",
            "Epoch 13/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2448 - loss: 4.0638\n",
            "Epoch 14/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2631 - loss: 4.0608\n",
            "Epoch 15/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2684 - loss: 4.0581\n",
            "Epoch 16/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2579 - loss: 4.0560\n",
            "Epoch 17/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2894 - loss: 4.0527\n",
            "Epoch 18/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2946 - loss: 4.0498\n",
            "Epoch 19/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2999 - loss: 4.0474\n",
            "Epoch 20/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2842 - loss: 4.0441\n",
            "Epoch 21/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2711 - loss: 4.0429\n",
            "Epoch 22/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2579 - loss: 4.0377\n",
            "Epoch 23/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2736 - loss: 4.0358\n",
            "Epoch 24/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2607 - loss: 4.0334 \n",
            "Epoch 25/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2659 - loss: 4.0294\n",
            "Epoch 26/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2946 - loss: 4.0264\n",
            "Epoch 27/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3026 - loss: 4.0231\n",
            "Epoch 28/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2817 - loss: 4.0204\n",
            "Epoch 29/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2870 - loss: 4.0167\n",
            "Epoch 30/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2870 - loss: 4.0141\n",
            "\n",
            "Prediction Example:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e21b3d6f9c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "serial\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "driver\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "#   1) LOAD TEXT FROM DOCUMENT\n",
        "# ============================================\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# read text from file\n",
        "with open(\"CBOW.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Loaded text length:\", len(text))\n",
        "\n",
        "# ============================================\n",
        "#   2) PREPROCESS TEXT\n",
        "# ============================================\n",
        "sentences = sent_tokenize(text)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        tokenized = word_tokenize(sentence.lower())\n",
        "        filtered = [w for w in tokenized if w.isalnum() and w not in stop_words]\n",
        "        words.extend(filtered)\n",
        "    return words\n",
        "\n",
        "words = preprocess(sentences)\n",
        "print(\"Total words:\", len(words))\n",
        "\n",
        "# create vocabulary\n",
        "vocab = list(set(words))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# mapping words <-> indices\n",
        "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "# convert all words to indices\n",
        "indexed_words = [word_to_index[w] for w in words]\n",
        "\n",
        "# ============================================\n",
        "#   3) GENERATE CONTEXT–TARGET PAIRS (CBOW)\n",
        "# ============================================\n",
        "def generate_cbow_pairs(words_idx, window=2):\n",
        "    #Predicts the center (target) word from surrounding (context) words.\n",
        "    context_target = []\n",
        "    for i in range(window, len(words_idx) - window):\n",
        "        context = words_idx[i-window:i] + words_idx[i+1:i+1+window]\n",
        "        target = words_idx[i]\n",
        "        context_target.append((context, target))\n",
        "    return context_target\n",
        "\n",
        "pairs = generate_cbow_pairs(indexed_words, window=2)\n",
        "\n",
        "print(\"Example Pair:\", pairs[0])  # (context_indices, target_index)\n",
        "\n",
        "# split into X, y\n",
        "# contexts = 2D array where each row contains 4 context word indices\n",
        "contexts, targets = zip(*pairs)\n",
        "contexts = np.array(contexts)\n",
        "targets = to_categorical(targets, num_classes=vocab_size)\n",
        "\n",
        "# ============================================\n",
        "#   4) BUILD CBOW MODEL\n",
        "# ============================================\n",
        "embedding_dim = 20  # you can increase\n",
        "\n",
        "model = Sequential()\n",
        "# input_dim=vocab_size — vocabulary size (total unique words)\n",
        "# output_dim=embedding_dim — each word becomes a 20-dimensional vector\n",
        "# input_length=4 — expects 4 word indices (2 left context + 2 right context)\n",
        "# What it does:\n",
        "\n",
        "# Converts each word index (0–vocab_size) into a dense embedding vector (20-dim)\n",
        "# Input shape: (batch_size, 4) → indices of 4 context words\n",
        "# Output shape: (batch_size, 4, 20) → 4 words, each as 20-dim vector\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=4))  # 2 left, 2 right\n",
        "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))  # average embeddings\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "#Adam optimizer — adapts learning rate automaticall\n",
        "# ============================================\n",
        "#   5) TRAIN\n",
        "# ============================================\n",
        "model.fit(contexts, targets, epochs=30, batch_size=64)\n",
        "\n",
        "# ============================================\n",
        "#   6) TEST: PREDICT A MISSING WORD\n",
        "# ============================================\n",
        "def predict_missing_word(context_words):\n",
        "    ctx_idx = [word_to_index[w] for w in context_words]\n",
        "    ctx_idx = np.array(ctx_idx).reshape(1,4)\n",
        "    pred = model.predict(ctx_idx)[0]\n",
        "    predicted_word = index_to_word[np.argmax(pred)]\n",
        "    return predicted_word\n",
        "\n",
        "print(\"\\nPrediction Example:\")\n",
        "# The speed of transmission is an important point of difference between the two viruses\n",
        "# Influenza has a shorter median incubation period (the time from infection to appearance of symptoms)\n",
        "print(predict_missing_word([\"transmission\", \"important\", \"point\", \"difference\"]))\n",
        "print(predict_missing_word([\"incubation\", \"period\", \"infection\", \"symptoms\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input contexts: (batch, 4) \n",
        "  ↓\n",
        "Embedding → (batch, 4, 20)  [4 words, each 20-dim]\n",
        "  ↓\n",
        "Lambda/Mean → (batch, 20)   [averaged context vector]\n",
        "  ↓\n",
        "Dense Softmax → (batch, vocab_size)  [probability per word]\n",
        "  ↓\n",
        "argmax → predicted word index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Continuous Bag of Words (CBOW) model is a neural embedding technique used to learn dense vector representations of words based on their surrounding context. The goal of CBOW is to predict a target word given its nearby context words. During training, the model takes multiple context words (within a fixed window size), converts them into embeddings, averages these embeddings to generate a context representation, and then feeds this into a softmax classifier to predict the center word. By minimizing categorical cross-entropy loss over large corpora, CBOW learns semantic relationships, where words appearing in similar contexts end up with similar embedding vectors. This enables the model to capture syntactic and semantic meaning such as analogies, similarity, and clustering. CBOW is computationally efficient and forms the foundation of the Word2Vec architecture used in many NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWegQkvA9Xv6"
      },
      "source": [
        "✅ 1. Core Theory: What is CBOW? (Exam-Perfect Explanation)\n",
        "\n",
        "CBOW (Continuous Bag of Words) is a Word2Vec model that:\n",
        "\n",
        "Takes context words around a missing word\n",
        "\n",
        "And predicts the target word in the center\n",
        "\n",
        "Learns dense vector embeddings by training on many context–target pairs\n",
        "\n",
        "Uses an embedding layer to convert words to vectors\n",
        "\n",
        "Uses averaging of context embeddings\n",
        "\n",
        "Uses softmax to predict the missing word\n",
        "\n",
        "Learns semantic meaning: words used in similar contexts get similar vectors\n",
        "\n",
        "In simple terms:\n",
        "Feed the context → model guesses the missing word.\n",
        "The model becomes good at this, and that’s how it learns word embeddings.\n",
        "\n",
        "✅ 2. Code Explanation (Very Detailed & Clean)\n",
        "\n",
        "Let’s go section-by-section.\n",
        "\n",
        "✅ 1) Loading Text\n",
        "text = open(\"CBOW.txt\").read()\n",
        "\n",
        "\n",
        "Reads the textual document.\n",
        "\n",
        "NLTK is used for tokenization and stopwords.\n",
        "\n",
        "✅ 2) Preprocessing the Text\n",
        "Steps done:\n",
        "\n",
        "Sentence tokenization\n",
        "\n",
        "Word tokenization\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Remove stopwords (the, is, are…)\n",
        "\n",
        "Remove non-alphanumeric junk\n",
        "\n",
        "Create a clean list of useful words\n",
        "\n",
        "This builds your corpus.\n",
        "\n",
        "Vocabulary Creation\n",
        "vocab = list(set(words))\n",
        "\n",
        "\n",
        "Makes list of all unique words.\n",
        "\n",
        "Mapping word ↔ index created.\n",
        "\n",
        "This is essential for embedding lookup.\n",
        "\n",
        "✅ 3) Generate CBOW Training Data\n",
        "\n",
        "CBOW works like this:\n",
        "\n",
        "context → target\n",
        "\n",
        "\n",
        "If window = 2:\n",
        "\n",
        "context: w1 w2   [target word]   w3 w4\n",
        "\n",
        "\n",
        "The code:\n",
        "\n",
        "for i in range(window, len(words_idx)-window):\n",
        "    context = words_idx[i-window:i] + words_idx[i+1:i+1+window]\n",
        "    target  = words_idx[i]\n",
        "\n",
        "\n",
        "So for:\n",
        "\n",
        "I love deep learning models\n",
        "\n",
        "\n",
        "with window 2:\n",
        "\n",
        "context: [\"I\", \"love\", \"learning\", \"models\"]\n",
        "target:  \"deep\"\n",
        "\n",
        "\n",
        "Then contexts → array of size (N,4)\n",
        "Targets → one-hot vectors of vocabulary size.\n",
        "\n",
        "This is the most important exam concept.\n",
        "\n",
        "✅ 4) Building CBOW Model\n",
        "\n",
        "Architecture:\n",
        "\n",
        "✅ Embedding Layer\n",
        "Embedding(vocab_size, 20)\n",
        "\n",
        "\n",
        "Converts each word index → dense embedding vector (20-dim)\n",
        "\n",
        "✅ Lambda (Mean) Layer\n",
        "tf.reduce_mean(x, axis=1)\n",
        "\n",
        "\n",
        "Takes 4 embeddings\n",
        "\n",
        "Averages them\n",
        "\n",
        "Produces Context Vector\n",
        "This is the CBOW trick: \"Bag of Words\" → order doesn’t matter.\n",
        "\n",
        "✅ Dense Softmax Layer\n",
        "Dense(vocab_size, activation='softmax')\n",
        "\n",
        "\n",
        "Predicts which word in the vocabulary is the center word.\n",
        "\n",
        "✅ Loss + Optimizer\n",
        "\n",
        "Uses categorical cross entropy because output = probability distribution.\n",
        "\n",
        "Adam optimizer for stable training.\n",
        "\n",
        "✅ 5) Training\n",
        "model.fit(contexts, targets)\n",
        "\n",
        "\n",
        "The model learns to guess the missing center word.\n",
        "\n",
        "During training, embeddings get meaningful.\n",
        "\n",
        "✅ 6) Predict Missing Word\n",
        "\n",
        "You give 4 context words:\n",
        "\n",
        "[\"transmission\", \"important\", \"viruses\", \"difference\"]\n",
        "\n",
        "\n",
        "Model outputs the most likely missing center word.\n",
        "\n",
        "✅ 3. Most Important Terminologies (Exam Gold)\n",
        "Term\tMeaning\n",
        "CBOW\tPredicts center word using context words\n",
        "Context Window\tNumber of words around the target\n",
        "Vocabulary\tAll unique words in corpus\n",
        "Embedding layer\tConverts words → dense vectors\n",
        "One-hot encoding\tTarget word in vector form\n",
        "Softmax\tPredicts probability of each word\n",
        "Context averaging\tCBOW’s main mechanism\n",
        "Training pair\t(context, target)\n",
        "Word2Vec\tEmbedding learning framework\n",
        "Latent Embedding Space\tMeaningful vector space of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfkMiZai67Ig"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
