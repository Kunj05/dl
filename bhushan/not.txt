✅ 1) Feedforward Neural Network (FNN) on MNIST — Theory

A Feedforward Neural Network (FNN) is the simplest neural network architecture where information moves in one direction: input → hidden layers → output. For MNIST digit classification, pixel values of the 28×28 image are flattened into a 784-dimensional vector and fed into dense layers. These layers apply learned weights and nonlinear activations (like ReLU) to map pixel patterns to digit classes. Since the task is multiclass classification (0–9), the output layer uses softmax activation to generate probability distribution across 10 classes. During training, categorical cross-entropy loss is minimized using an optimizer such as SGD or Adam. The network gradually learns digit-specific patterns like edges, curves, and pixel intensity clusters. Although FNNs work for MNIST, they are less efficient for image spatial structure because they ignore local neighborhood information.

✅ 2) Feedforward Neural Network (FNN) on CIFAR-10 — Theory

CIFAR-10 is a 32×32×3 RGB dataset, making it more complex than grayscale MNIST. FNNs flatten all pixels into a long vector (3072 values), losing spatial relationships. The model consists of multiple dense layers that try to learn global correlations between all input pixels and output classes. Due to lack of convolution filters, FNNs struggle with image-specific features and require more neurons to approximate patterns. Nevertheless, using activation functions like ReLU and softmax, and training with categorical cross-entropy, the FNN still performs basic image classification. CIFAR-10 demonstrates the limitation of FNNs on images and motivates the need for CNNs.

✅ 3) Convolutional Neural Network (CNN) on MNIST — Theory

A Convolutional Neural Network is specifically designed for images. Instead of flattening, CNNs use convolution layers to detect spatial patterns such as edges, corners, textures, and shapes. MNIST images (28×28×1) are passed through convolution layers with filters that learn digit-specific features. MaxPooling reduces spatial resolution while preserving important information, improving computation efficiency. The Flatten layer converts feature maps into a 1D vector that flows into dense layers for classification. Softmax outputs the predicted digit class. CNNs outperform FNNs because they exploit local spatial structure and translation invariance, making them ideal for digit recognition.

✅ 4) Convolutional Neural Network (CNN) on CIFAR-10 — Theory

For CIFAR-10, CNNs extract hierarchical visual features from RGB images. Early convolution layers learn low-level features like edges and color gradients. Deeper layers learn object-level features such as shapes and textures. Pooling layers reduce dimensionality and enhance generalization. The classifier at the end uses dense layers with softmax to output probabilities for 10 classes. Training uses categorical cross-entropy and an optimizer like Adam. CNNs achieve high accuracy on CIFAR-10 because they preserve spatial relationships, support multi-channel learning, and extract robust features through convolution kernels.

✅ 5) Autoencoder & Encoder (Anomaly Detection) — Theory

An autoencoder is an unsupervised neural architecture consisting of an encoder and a decoder. The encoder compresses high-dimensional input into a low-dimensional latent vector by learning essential structure and removing noise. The decoder reconstructs input from this compressed representation. During anomaly detection, the autoencoder is trained only on normal data, enabling it to learn the distribution and patterns of normal samples. When an abnormal sample is passed through the encoder–decoder pipeline, reconstruction fails, producing a high reconstruction error measured using MSE. A statistical threshold (mean + 3σ) separates normal from anomalous samples. Autoencoders are powerful for anomaly detection because they learn compact, meaningful representations that highlight deviations in unseen data.

✅ 6) Continuous Bag of Words (CBOW) — Theory

CBOW is a Word2Vec model used to learn dense vector embeddings of words based on context. Its objective is to predict the target word using surrounding context words. After preprocessing text (tokenization, stopword removal), CBOW converts words into integer indices and forms context–target training pairs using a fixed window size. The embeddings of context words are averaged to form a combined context vector, which is fed into a softmax classifier that predicts the most likely target word. The model is trained using categorical cross-entropy. Over time, CBOW learns semantic and syntactic relationships, placing similar words close in embedding space. CBOW is computationally efficient and fundamental to modern NLP tasks.

✅ 7) Object Detection / Image Classification using Transfer Learning (VGG16) — Theory

Transfer learning leverages pre-trained CNNs, such as VGG16, trained on large datasets like ImageNet. These models have already learned robust visual features such as edges, textures, and object shapes. We load the pre-trained convolution layers (feature extractor) and freeze them to prevent weight updates. On top of this frozen base, we attach a custom classifier trained on our smaller dataset. After initial training, fine-tuning is performed by unfreezing the top convolution layers and updating them with a low learning rate, allowing the model to adapt high-level features to the new task. This two-stage approach (feature extraction → fine-tuning) significantly improves accuracy while reducing training time and overfitting. It is widely used for object classification and detection tasks with limited data.